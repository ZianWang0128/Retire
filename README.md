# Retire
Estimation and inference methods for robust expectile (robust asymmetric least squares) regression ([Man et al., 2022](https://arxiv.org/abs/2212.05562)):
$$\hat{\beta} \in {\rm{argmin}} \frac{1}{n}\sum_{i=1}^n L_{\tau, \gamma}(y_i - x_i^T \beta) + P(\beta), $$
where $L_{\tau, \gamma}(u) = |\tau - 1(u<0)| \cdot \ell_\gamma(u)$ is the robust expectile loss typified by the asymmetric Huber loss, and $P(\beta)$ can be either convex or nonconvex penaltis, e.g., weighted $L_1$ penalty, elastic net ([Zou & Hastie, 2005](https://www.jstor.org/stable/3647580)), (sparse) group lasso penalty ([Yuan & Lin, 2006](https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9868.2005.00532.x), [Simon et al., 2013](https://www.tandfonline.com/doi/abs/10.1080/10618600.2012.681250)), SCAD penalty ([Fan & Li, 2001](https://www.tandfonline.com/doi/abs/10.1198/016214501753382273)), capped $L_1$ penalty and minimax concave penalty (MCP) ([Zhang, 2010](https://projecteuclid.org/journals/annals-of-statistics/volume-38/issue-2/Nearly-unbiased-variable-selection-under-minimax-concave-penalty/10.1214/09-AOS729.full)).
Under high dimensions ($n \ll p$), the iterative local adaptive majorize-minimize (ILAMM) algorithm is employed for computing robust expectile regression with convex penalties, while nonconvex penalties are handled by the local linear approximation (LLA) algorithm proposed by [Zou & Li (2008)](https://projecteuclid.org/journals/annals-of-statistics/volume-36/issue-4/One-step-sparse-estimates-in-nonconcave-penalized-likelihood-models/10.1214/009053607000000802.full). Under low dimensions ($n > p$), the gradient descent algorithm with Barzilai-Borwein stepsize ([Barzilai & Browein, 1988](https://academic.oup.com/imajna/article-abstract/8/1/141/802460)) is employed. 
